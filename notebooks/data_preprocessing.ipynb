{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "from nlp_dauphine.preprocess_data import load_data, text_cleaning, concatenate_texts\n",
    "from nlp_dauphine.embeddings import vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('c:\\\\Users\\\\Hugo\\\\Documents\\\\Travail\\\\A5\\\\Dauphine_challenge\\\\nlp_dauphine')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"data\")\n",
    "files_path = {\n",
    "    \"ecb\": Path(path,\"ecb_data.csv\"),\n",
    "    \"fed\": Path(path,\"fed_data.csv\"),\n",
    "    \"train_series\": Path(path,\"train_series.csv\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1772, 3) (739, 3) (8946, 14)\n"
     ]
    }
   ],
   "source": [
    "# LOAD TEXT DATA\n",
    "df_ecb, df_fed, df_train_series = load_data(files_path)\n",
    "print(df_ecb.shape, df_fed.shape, df_train_series.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Footnotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal with Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLACE NULL TEXTS BY TITLES\n",
    "index_no_texts = df_ecb[df_ecb[\"text\"].isnull()].index.values\n",
    "df_ecb.loc[index_no_texts, \"text\"] = df_ecb.loc[index_no_texts, \"title\"].values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove special characters, stop words, Lemmatization..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET TEXTS ONLY\n",
    "txt_fed = df_fed.text.values\n",
    "txt_ecb = df_ecb.text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# CLEAN TEXTS\u001b[39;00m\n\u001b[0;32m      2\u001b[0m negation_words \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mno\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnot\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m txt_fed \u001b[39m=\u001b[39m [\n\u001b[0;32m      4\u001b[0m     text_cleaning(\n\u001b[0;32m      5\u001b[0m         txt, negation_set\u001b[39m=\u001b[39mnegation_words, fg_stop_words\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, fg_lemmatization\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m      6\u001b[0m     )\n\u001b[0;32m      7\u001b[0m     \u001b[39mfor\u001b[39;00m txt \u001b[39min\u001b[39;00m txt_fed\n\u001b[0;32m      8\u001b[0m ]\n\u001b[0;32m      9\u001b[0m txt_ecb \u001b[39m=\u001b[39m [\n\u001b[0;32m     10\u001b[0m     text_cleaning(\n\u001b[0;32m     11\u001b[0m         txt, negation_set\u001b[39m=\u001b[39mnegation_words, fg_stop_words\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, fg_lemmatization\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     12\u001b[0m     )\n\u001b[0;32m     13\u001b[0m     \u001b[39mfor\u001b[39;00m txt \u001b[39min\u001b[39;00m txt_ecb\n\u001b[0;32m     14\u001b[0m ]\n",
      "Cell \u001b[1;32mIn[30], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# CLEAN TEXTS\u001b[39;00m\n\u001b[0;32m      2\u001b[0m negation_words \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mno\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnot\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m      3\u001b[0m txt_fed \u001b[39m=\u001b[39m [\n\u001b[1;32m----> 4\u001b[0m     text_cleaning(\n\u001b[0;32m      5\u001b[0m         txt, negation_set\u001b[39m=\u001b[39;49mnegation_words, fg_stop_words\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, fg_lemmatization\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m      6\u001b[0m     )\n\u001b[0;32m      7\u001b[0m     \u001b[39mfor\u001b[39;00m txt \u001b[39min\u001b[39;00m txt_fed\n\u001b[0;32m      8\u001b[0m ]\n\u001b[0;32m      9\u001b[0m txt_ecb \u001b[39m=\u001b[39m [\n\u001b[0;32m     10\u001b[0m     text_cleaning(\n\u001b[0;32m     11\u001b[0m         txt, negation_set\u001b[39m=\u001b[39mnegation_words, fg_stop_words\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, fg_lemmatization\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     12\u001b[0m     )\n\u001b[0;32m     13\u001b[0m     \u001b[39mfor\u001b[39;00m txt \u001b[39min\u001b[39;00m txt_ecb\n\u001b[0;32m     14\u001b[0m ]\n",
      "File \u001b[1;32m~\\Documents\\Travail\\A5\\Dauphine_challenge\\nlp_dauphine\\src\\nlp_dauphine\\load_clean_data.py:96\u001b[0m, in \u001b[0;36mtext_cleaning\u001b[1;34m(corpus, negation_set, fg_stop_words, fg_lemmatization)\u001b[0m\n\u001b[0;32m     92\u001b[0m     corpus_words \u001b[39m=\u001b[39m [word \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m corpus_words \u001b[39mif\u001b[39;00m word \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stop_words]\n\u001b[0;32m     94\u001b[0m \u001b[39mif\u001b[39;00m fg_lemmatization:\n\u001b[0;32m     95\u001b[0m     \u001b[39m# lemmatization\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m     corpus_pos_tag \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mtag\u001b[39m.\u001b[39;49mpos_tag(corpus_words)\n\u001b[0;32m     97\u001b[0m     corpus_pos_tag \u001b[39m=\u001b[39m [\n\u001b[0;32m     98\u001b[0m         (word, get_wordnet_pos(pos_tag)) \u001b[39mfor\u001b[39;00m (word, pos_tag) \u001b[39min\u001b[39;00m corpus_pos_tag\n\u001b[0;32m     99\u001b[0m     ]\n\u001b[0;32m    100\u001b[0m     wordnet_lemmatizer \u001b[39m=\u001b[39m WordNetLemmatizer()\n",
      "File \u001b[1;32mc:\\Users\\Hugo\\anaconda3\\envs\\nlp_dauph_env\\lib\\site-packages\\nltk\\tag\\__init__.py:166\u001b[0m, in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[39mUse NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39mtag the given list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[39m:rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    165\u001b[0m tagger \u001b[39m=\u001b[39m _get_tagger(lang)\n\u001b[1;32m--> 166\u001b[0m \u001b[39mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[1;32mc:\\Users\\Hugo\\anaconda3\\envs\\nlp_dauph_env\\lib\\site-packages\\nltk\\tag\\__init__.py:123\u001b[0m, in \u001b[0;36m_pos_tag\u001b[1;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mtokens: expected a list of strings, got a string\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    122\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 123\u001b[0m     tagged_tokens \u001b[39m=\u001b[39m tagger\u001b[39m.\u001b[39;49mtag(tokens)\n\u001b[0;32m    124\u001b[0m     \u001b[39mif\u001b[39;00m tagset:  \u001b[39m# Maps to the specified tagset.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         \u001b[39mif\u001b[39;00m lang \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39meng\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Hugo\\anaconda3\\envs\\nlp_dauph_env\\lib\\site-packages\\nltk\\tag\\perceptron.py:187\u001b[0m, in \u001b[0;36mPerceptronTagger.tag\u001b[1;34m(self, tokens, return_conf, use_tagdict)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tag:\n\u001b[0;32m    186\u001b[0m     features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_features(i, word, context, prev, prev2)\n\u001b[1;32m--> 187\u001b[0m     tag, conf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(features, return_conf)\n\u001b[0;32m    188\u001b[0m output\u001b[39m.\u001b[39mappend((word, tag, conf) \u001b[39mif\u001b[39;00m return_conf \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m \u001b[39melse\u001b[39;00m (word, tag))\n\u001b[0;32m    190\u001b[0m prev2 \u001b[39m=\u001b[39m prev\n",
      "File \u001b[1;32mc:\\Users\\Hugo\\anaconda3\\envs\\nlp_dauph_env\\lib\\site-packages\\nltk\\tag\\perceptron.py:69\u001b[0m, in \u001b[0;36mAveragedPerceptron.predict\u001b[1;34m(self, features, return_conf)\u001b[0m\n\u001b[0;32m     66\u001b[0m         scores[label] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m value \u001b[39m*\u001b[39m weight\n\u001b[0;32m     68\u001b[0m \u001b[39m# Do a secondary alphabetic sort, for stability\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m best_label \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclasses, key\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m label: (scores[label], label))\n\u001b[0;32m     70\u001b[0m \u001b[39m# compute the confidence\u001b[39;00m\n\u001b[0;32m     71\u001b[0m conf \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_softmax(scores)) \u001b[39mif\u001b[39;00m return_conf \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Hugo\\anaconda3\\envs\\nlp_dauph_env\\lib\\site-packages\\nltk\\tag\\perceptron.py:69\u001b[0m, in \u001b[0;36mAveragedPerceptron.predict.<locals>.<lambda>\u001b[1;34m(label)\u001b[0m\n\u001b[0;32m     66\u001b[0m         scores[label] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m value \u001b[39m*\u001b[39m weight\n\u001b[0;32m     68\u001b[0m \u001b[39m# Do a secondary alphabetic sort, for stability\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m best_label \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses, key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m label: (scores[label], label))\n\u001b[0;32m     70\u001b[0m \u001b[39m# compute the confidence\u001b[39;00m\n\u001b[0;32m     71\u001b[0m conf \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_softmax(scores)) \u001b[39mif\u001b[39;00m return_conf \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# CLEAN TEXTS\n",
    "negation_words = [\"no\", \"not\"]\n",
    "txt_fed = [\n",
    "    text_cleaning(\n",
    "        txt, negation_set=negation_words, fg_stop_words=True, fg_lemmatization=True,\n",
    "    )\n",
    "    for txt in txt_fed\n",
    "]\n",
    "txt_ecb = [\n",
    "    text_cleaning(\n",
    "        txt, negation_set=negation_words, fg_stop_words=True, fg_lemmatization=True,\n",
    "    )\n",
    "    for txt in txt_ecb\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>speaker</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Comments by Yves Mersch at Financial Services ...</td>\n",
       "      <td>Yves Mersch</td>\n",
       "      <td>Comments by Yves Mersch at Financial Service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Securing sustained economic growth in the euro...</td>\n",
       "      <td>Vítor Constâncio</td>\n",
       "      <td>Securing sustained economic growth in the eu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The role of monetary policy in addressing the ...</td>\n",
       "      <td>Mario Draghi</td>\n",
       "      <td>The role of monetary policy in addressing th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The pandemic emergency: the three challenges f...</td>\n",
       "      <td>Philip R. Lane</td>\n",
       "      <td>SPEECH  The pandemic emergency: the three c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Transmission channels of monetary policy in th...</td>\n",
       "      <td>Peter Praet</td>\n",
       "      <td>Transmission channels of monetary policy in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1767</th>\n",
       "      <td>Navigating uncertainty - governance, risk mana...</td>\n",
       "      <td>Sabine Lautenschläger</td>\n",
       "      <td>Navigating uncertainty – governance, risk ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768</th>\n",
       "      <td>3-6-3: Banks and change</td>\n",
       "      <td>Sabine Lautenschläger</td>\n",
       "      <td>3-6-3: Banks and change   Speech by Sabine L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769</th>\n",
       "      <td>Europe's pursuit of ‘a more perfect Union'</td>\n",
       "      <td>Mario Draghi</td>\n",
       "      <td>Europe’s pursuit of ‘a more perfect Union’  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1770</th>\n",
       "      <td>Monetary policy in the euro area - a brief ass...</td>\n",
       "      <td>Yves Mersch</td>\n",
       "      <td>Monetary policy in the euro area - a brief a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1771</th>\n",
       "      <td>The ECB and its role as lender of last resort ...</td>\n",
       "      <td>Peter Praet</td>\n",
       "      <td>The ECB and its role as lender of last resor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1772 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0     Comments by Yves Mersch at Financial Services ...   \n",
       "1     Securing sustained economic growth in the euro...   \n",
       "2     The role of monetary policy in addressing the ...   \n",
       "3     The pandemic emergency: the three challenges f...   \n",
       "4     Transmission channels of monetary policy in th...   \n",
       "...                                                 ...   \n",
       "1767  Navigating uncertainty - governance, risk mana...   \n",
       "1768                            3-6-3: Banks and change   \n",
       "1769         Europe's pursuit of ‘a more perfect Union'   \n",
       "1770  Monetary policy in the euro area - a brief ass...   \n",
       "1771  The ECB and its role as lender of last resort ...   \n",
       "\n",
       "                    speaker                                               text  \n",
       "0               Yves Mersch    Comments by Yves Mersch at Financial Service...  \n",
       "1          Vítor Constâncio    Securing sustained economic growth in the eu...  \n",
       "2              Mario Draghi    The role of monetary policy in addressing th...  \n",
       "3            Philip R. Lane     SPEECH  The pandemic emergency: the three c...  \n",
       "4               Peter Praet    Transmission channels of monetary policy in ...  \n",
       "...                     ...                                                ...  \n",
       "1767  Sabine Lautenschläger    Navigating uncertainty – governance, risk ma...  \n",
       "1768  Sabine Lautenschläger    3-6-3: Banks and change   Speech by Sabine L...  \n",
       "1769           Mario Draghi    Europe’s pursuit of ‘a more perfect Union’  ...  \n",
       "1770            Yves Mersch    Monetary policy in the euro area - a brief a...  \n",
       "1771            Peter Praet    The ECB and its role as lender of last resor...  \n",
       "\n",
       "[1772 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ecb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the co-occurence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings from the co-occurence Matrix\n",
    "\n",
    "### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a51b8630f094c97fcc2dad78dd00dbbee2b9ea2d8294aad785600076506e9b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
